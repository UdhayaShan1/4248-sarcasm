{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcastic headlines: 13634\n",
      "Non-sarcastic headlines: 14985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 427/427 [00:15<00:00, 27.04it/s]\n",
      "Batches: 100%|██████████| 469/469 [00:16<00:00, 28.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best sarcastic match for each non-sarcastic headline...\n",
      "\n",
      "✅ Sample pairs:\n",
      "                                               input  \\\n",
      "0  sarcasmify: dem rep. totally nails why congres...   \n",
      "1  sarcasmify: eat your veggies: 9 deliciously di...   \n",
      "2                   sarcasmify: my white inheritance   \n",
      "3  sarcasmify: 5 ways to file your taxes with les...   \n",
      "4     sarcasmify: lots of parents know this scenario   \n",
      "\n",
      "                                              target  \n",
      "0  presumptuous congressional freshman thinks she...  \n",
      "1    vegetarian option just iceberg lettuce on bread  \n",
      "2  report: all the other races coming to take you...  \n",
      "3  woman going to take quick break after filling ...  \n",
      "4  new babysitter can already tell this kind of k...  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# ----------- Step 1: Load Dataset -----------\n",
    "def parse_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = parse_data('Sarcasm_Headlines_Dataset_v2.json')\n",
    "\n",
    "# ----------- Step 2: Split by Sarcasm Label -----------\n",
    "df_sarcastic = df[df['is_sarcastic'] == 1].reset_index(drop=True)\n",
    "df_non_sarcastic = df[df['is_sarcastic'] == 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Sarcastic headlines: {len(df_sarcastic)}\")\n",
    "print(f\"Non-sarcastic headlines: {len(df_non_sarcastic)}\")\n",
    "\n",
    "# ----------- Step 3: Load SentenceTransformer Model -----------\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# ----------- Step 4: Encode Headlines -----------\n",
    "sarcastic_embeddings = model.encode(df_sarcastic['headline'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "non_sarcastic_embeddings = model.encode(df_non_sarcastic['headline'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# ----------- Step 5: Semantic Search to Create Pairs -----------\n",
    "print(\"Finding best sarcastic match for each non-sarcastic headline...\")\n",
    "\n",
    "hits = util.semantic_search(non_sarcastic_embeddings, sarcastic_embeddings, top_k=1)\n",
    "pseudo_pairs = []\n",
    "\n",
    "for i, hit in enumerate(hits):\n",
    "    target_idx = hit[0]['corpus_id']\n",
    "    pseudo_pairs.append({\n",
    "        'input': df_non_sarcastic.iloc[i]['headline'],\n",
    "        'target': df_sarcastic.iloc[target_idx]['headline']\n",
    "    })\n",
    "\n",
    "df_pairs = pd.DataFrame(pseudo_pairs)\n",
    "df_pairs['input'] = 'sarcasmify: ' + df_pairs['input']\n",
    "\n",
    "# ----------- Step 6: Save or Display Sample -----------\n",
    "df_pairs.to_csv(\"pseudo_sarcasm_pairs.csv\", index=False)\n",
    "print(\"\\n✅ Sample pairs:\")\n",
    "print(df_pairs.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df_pairs).train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================================\n",
    "# ⚙️ TOKENIZER & TRAINING FUNCTION\n",
    "# ======================================\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "def train_model(model_name, model_class, tokenizer_class, dataset, epoch = 3, dataset_name = \"ori\"):\n",
    "    model = model_class.from_pretrained(model_name)\n",
    "    tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess(examples):\n",
    "        inputs = tokenizer(examples[\"input\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "        targets = tokenizer(examples[\"target\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "        inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "        return inputs\n",
    "\n",
    "    tokenized = dataset.map(preprocess, batched=True)\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./{model_name.replace('/', '_')}_{dataset_name}_sarcasm\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=epoch,\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=2,\n",
    "        logging_dir=\"./logs\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized[\"train\"],\n",
    "        eval_dataset=tokenized[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "    )\n",
    "    trainer.train()\n",
    "    return model, tokenizer\n",
    "\n",
    "# ======================================\n",
    "# 🧠 TRAIN MODELS\n",
    "# ======================================\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration, T5Tokenizer,\n",
    "    BartForConditionalGeneration, BartTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13486/13486 [00:01<00:00, 11817.00 examples/s]\n",
      "Map: 100%|██████████| 1499/1499 [00:00<00:00, 11827.52 examples/s]\n",
      "/Users/samuelthen/Documents/CS4248/cs4248-project/.venv/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/zr/tc7r4hpj6tscx9pf08qck6m80000gn/T/ipykernel_27705/1849639684.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5058' max='5058' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5058/5058 10:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.100500</td>\n",
       "      <td>0.976549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.023500</td>\n",
       "      <td>0.920436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.970200</td>\n",
       "      <td>0.901305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t5_model, t5_tokenizer = train_model(\"t5-small\", T5ForConditionalGeneration, T5Tokenizer, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13486/13486 [00:01<00:00, 7465.33 examples/s]\n",
      "Map: 100%|██████████| 1499/1499 [00:00<00:00, 5885.26 examples/s]\n",
      "/Users/samuelthen/Documents/CS4248/cs4248-project/.venv/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/zr/tc7r4hpj6tscx9pf08qck6m80000gn/T/ipykernel_27705/1849639684.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5058' max='5058' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5058/5058 22:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>0.605554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.446600</td>\n",
       "      <td>0.461681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.436526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelthen/Documents/CS4248/cs4248-project/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3353: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bart_model, bart_tokenizer = train_model(\"facebook/bart-base\", BartForConditionalGeneration, BartTokenizer, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13486/13486 [00:01<00:00, 12025.79 examples/s]\n",
      "Map: 100%|██████████| 1499/1499 [00:00<00:00, 11980.69 examples/s]\n",
      "/Users/samuelthen/Documents/CS4248/cs4248-project/.venv/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/zr/tc7r4hpj6tscx9pf08qck6m80000gn/T/ipykernel_27705/1849639684.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5058' max='5058' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5058/5058 12:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.032400</td>\n",
       "      <td>0.894236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.926300</td>\n",
       "      <td>0.828817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.808039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flan_model, flan_tokenizer = train_model(\"google/flan-t5-small\", T5ForConditionalGeneration, T5Tokenizer, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# 🧠 GPT-2 MODEL (Separate Preprocessing)\n",
    "# ======================================\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def preprocess_gpt2(examples):\n",
    "    prompts = [inp + \". \" + tgt for inp, tgt in zip(examples[\"input\"], examples[\"target\"])]\n",
    "    model_inputs = gpt2_tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "gpt2_tokenized = dataset.map(preprocess_gpt2, batched=True, remove_columns=[\"input\", \"target\"])\n",
    "gpt2_collator = DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False)\n",
    "gpt2_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_sarcasm\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "gpt2_trainer = Trainer(\n",
    "    model=gpt2_model,\n",
    "    args=gpt2_args,\n",
    "    data_collator=gpt2_collator,\n",
    "    train_dataset=gpt2_tokenized[\"train\"],\n",
    ")\n",
    "gpt2_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `jaguar land rover pauses shipments to u.s. amid new tariffs`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `jaguar land rover announces plans to expand jaguar land rover` |\n",
       "| Bart      | `lone tent a dark harbinger of looming street festival` |\n",
       "| FLAN-T5   | `jaguar land rover announces plans to re-establish jaguar land rover` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `president trump's tariffs trigger global market turmoil`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `trump warns u.s. not to compromise on u.s. tariffs` |\n",
       "| Bart      | `world wonders what trump has on united states that's forcing nation to keep him in power` |\n",
       "| FLAN-T5   | `trump unveils plan to address climate change by reviving climate change` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `global reaction to trump's sweeping tariff measures`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `trump warns u.s. not to tolerate u.s. tariff hikes` |\n",
       "| Bart      | `trump warns china not to underestimate his willingness to sacrifice every american's well-being` |\n",
       "| FLAN-T5   | `trump unveils plan to address climate change by reviving climate change` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `potential worldwide recession feared due to u.s. tariffs`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `report: u.s. has no idea what to expect from u.s.` |\n",
       "| Bart      | `report: it not good time for long, devastating war for iran, either` |\n",
       "| FLAN-T5   | `u.s. warns americans not to underestimate u.s. economic woes` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `vietnam, india, and israel in talks over new trade deals`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `israel, vietnam, india, iran, israel, u.s., u.s., u.s., u.s., u.s., u.s., u.s., u.s.` |\n",
       "| Bart      | `israeli government found to be in league with jewry` |\n",
       "| FLAN-T5   | `israel, u.s., u.s., u.s., u.s., u.s., u.s., u.s., u.s.` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `u.s. supreme court allows freeze on dei-focused teacher training grants`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `supreme court grants dei-focused teacher training` |\n",
       "| Bart      | `supreme court to hear cases determining whether human beings deserve equal rights` |\n",
       "| FLAN-T5   | `u.s. supreme court approves dei-focused teacher training program` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `chevron ordered to pay $744 million for environmental damages`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `chevron to pay $744 million for environmental damage` |\n",
       "| Bart      | `epa rolls back emissions standards to increase consumer choice over type of apocalyptic hellscape earth will become` |\n",
       "| FLAN-T5   | `chevron announces plan to reduce emissions by 30% by 2050` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `surge in measles cases reported in texas`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `report: texas has more measles cases than other states` |\n",
       "| Bart      | `perverted measles virus exposes itself to playground full of children` |\n",
       "| FLAN-T5   | `measles outbreak a cry for help, vengeance against humanity` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `los angeles county settles $4 billion abuse case in juvenile facilities`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `california incarcerates dozens of juvenile juveniles` |\n",
       "| Bart      | `child abuse numbers under control, unless you count the super brutal ones` |\n",
       "| FLAN-T5   | `report: u.s. has no idea what it's like to be incarcerated` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `anti-trump protests erupt across the u.s.`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `trump slams u.s. anti-immigrant protests` |\n",
       "| Bart      | `peaceful protest interrupted by swarm of aggressive black-clad militants` |\n",
       "| FLAN-T5   | `u.s. condemns trump for attempting to rescind u.s.'s right to abortion` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `marsiling-yew tee to receive new community spaces and parks`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `tee taylor swift announces plans to reopen city hall` |\n",
       "| Bart      | `new park caters to business travelers` |\n",
       "| FLAN-T5   | `taylor swift announces plans to build new taylor swift stadium` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `political parties debate balance of foreign talent in singapore`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `fbi: 'it's a good time to think about it'` |\n",
       "| Bart      | `republicans, democrats unite in good laugh over reform party` |\n",
       "| FLAN-T5   | `nation's foreign policy experts recommend breaking down crushing defeats into smaller, more manageable failures` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `opel to cease vehicle sales in singapore by end-2025`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `opel announces plan to stop sales of cars in singapore` |\n",
       "| Bart      | `report: there never been a better time to buy than right now` |\n",
       "| FLAN-T5   | `opel announces plan to reduce emissions by 30% by 2025` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `singapore's economy projected to grow by 2.6% in 2025`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `report: singapore expected to grow by 2.6% in 2025` |\n",
       "| Bart      | `report: middle east quickly running out of land area for violence to spill over to` |\n",
       "| FLAN-T5   | `report: u.s. economy projected to grow by 2025` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `temporary bus diversions for singapore t100 2025 event`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `singapore t100 reopens for first time since 2001` |\n",
       "| Bart      | `bus transporting carnival cruise passengers crashes into sewage treatment plant` |\n",
       "| FLAN-T5   | `tuesday tuesday tuesday tuesday` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ======================================\n",
    "# 🎭 TEST ON SAMPLE HEADLINES\n",
    "# ======================================\n",
    "test_headlines = [\n",
    "    \"jaguar land rover pauses shipments to u.s. amid new tariffs\",\n",
    "    \"president trump's tariffs trigger global market turmoil\",\n",
    "    \"global reaction to trump's sweeping tariff measures\",\n",
    "    \"potential worldwide recession feared due to u.s. tariffs\",\n",
    "    \"vietnam, india, and israel in talks over new trade deals\",\n",
    "    \"u.s. supreme court allows freeze on dei-focused teacher training grants\",\n",
    "    \"chevron ordered to pay $744 million for environmental damages\",\n",
    "    \"surge in measles cases reported in texas\",\n",
    "    \"los angeles county settles $4 billion abuse case in juvenile facilities\",\n",
    "    \"anti-trump protests erupt across the u.s.\",\n",
    "    \"marsiling-yew tee to receive new community spaces and parks\",\n",
    "    \"political parties debate balance of foreign talent in singapore\",\n",
    "    \"opel to cease vehicle sales in singapore by end-2025\",\n",
    "    \"singapore's economy projected to grow by 2.6% in 2025\",\n",
    "    \"temporary bus diversions for singapore t100 2025 event\"\n",
    "]\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, headline, is_gpt2=False):\n",
    "    if is_gpt2:\n",
    "        model.to(\"cpu\")\n",
    "        prompt = f\"sarcasmify: {headline}.\"\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        output = model.generate(input_ids, max_length=64, do_sample=True, top_k=50, top_p=0.95, temperature=0.9, pad_token_id=tokenizer.eos_token_id)\n",
    "    else:\n",
    "        model.to(\"cpu\")\n",
    "        prompt = \"sarcasmify: \" + headline\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True)\n",
    "        output = model.generate(input_ids, max_length=64, num_beams=5, early_stopping=True)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# ======================================\n",
    "# 📊 COMPARE MODELS SIDE-BY-SIDE\n",
    "# ======================================\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "for h in test_headlines:\n",
    "    t5_out = generate(t5_model, t5_tokenizer, h)\n",
    "    bart_out = generate(bart_model, bart_tokenizer, h)\n",
    "    flan_out = generate(flan_model, flan_tokenizer, h)\n",
    "    # gpt2_out = generate(gpt2_model, gpt2_tokenizer, h, is_gpt2=True)\n",
    "\n",
    "    display(Markdown(f\"\"\"\n",
    "### 📰 Original: `{h}`  \n",
    "| Model | Output |\n",
    "|-------|--------|\n",
    "| T5        | `{t5_out}` |\n",
    "| Bart      | `{bart_out}` |\n",
    "| FLAN-T5   | `{flan_out}` |\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14985"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pseudo_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sarcasm: 100%|██████████| 14985/14985 [12:36:22<00:00,  3.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved enhanced sarcastic dataset with GPT-generated headlines:\n",
      "                                               input  \\\n",
      "0  sarcasmify: dem rep. totally nails why congres...   \n",
      "1  sarcasmify: eat your veggies: 9 deliciously di...   \n",
      "2                   sarcasmify: my white inheritance   \n",
      "3  sarcasmify: 5 ways to file your taxes with les...   \n",
      "4     sarcasmify: lots of parents know this scenario   \n",
      "\n",
      "                                           reference  \\\n",
      "0  presumptuous congressional freshman thinks she...   \n",
      "1    vegetarian option just iceberg lettuce on bread   \n",
      "2  report: all the other races coming to take you...   \n",
      "3  woman going to take quick break after filling ...   \n",
      "4  new babysitter can already tell this kind of k...   \n",
      "\n",
      "                                           generated  \n",
      "0  \"brilliant congresswoman suggests that actuall...  \n",
      "1  \"feast on your greens: 9 ways to make broccoli...  \n",
      "2  \"Study: Everyone's just dying to get their han...  \n",
      "3  \"5 Ingenious Techniques to Turn Your Tax Filin...  \n",
      "4  \"parents everywhere have mastered the art of p...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# ----------- Step 4: Generate New Sarcastic Headlines Using OpenAI -----------\n",
    "generated_data = []\n",
    "\n",
    "def create_prompt(pair):\n",
    "    return (\n",
    "        f\"Example sarcastic headline (style guide only): \\\"{pair['target']}\\\"\\n\"\n",
    "        f\"Non-sarcastic headline to convert: \\\"{pair['input']}\\\"\\n\\n\"\n",
    "        \"Write a sarcastic version of the non-sarcastic headline. \"\n",
    "        \"Do NOT copy or refer to the content of the example. \"\n",
    "        \"Only mimic its tone, structure, and sarcasm style.\\n\"\n",
    "        \"Only return the sarcastic headline. No explanation or intro.\"\n",
    "    )\n",
    "\n",
    "# Progress bar over the whole list\n",
    "pbar = tqdm(total=len(pseudo_pairs), desc=\"Generating sarcasm\")\n",
    "\n",
    "for idx, pair in enumerate(pseudo_pairs):\n",
    "    prompt = create_prompt(pair)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert sarcasm writer.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=60,\n",
    "            temperature=0.8\n",
    "        )\n",
    "        output = response.choices[0].message.content.strip()\n",
    "\n",
    "        generated_data.append({\n",
    "            \"input\": \"sarcasmify: \" + pair[\"input\"],\n",
    "            \"reference\": pair[\"target\"],\n",
    "            \"generated\": output\n",
    "        })\n",
    "        pbar.update(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error at index {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "    time.sleep(1)  # Respect rate limits\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# ----------- Step 5: Save Results -----------\n",
    "df_results = pd.DataFrame(generated_data)\n",
    "df_results.to_csv(\"gpt_sarcasm_augmented_dataset.csv\", index=False)\n",
    "print(\"\\n✅ Saved enhanced sarcastic dataset with GPT-generated headlines:\")\n",
    "print(df_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                      input                                                                                  reference                                                                                                                   generated\n",
      "sarcasmify: dem rep. totally nails why congress is falling short on gender, racial equality presumptuous congressional freshman thinks she can just come in and represent constituents \"brilliant congresswoman suggests that actually addressing gender and racial equality might be a good idea—groundbreaking!\"\n",
      "                              sarcasmify: eat your veggies: 9 deliciously different recipes                                            vegetarian option just iceberg lettuce on bread                                                      \"feast on your greens: 9 ways to make broccoli a culinary masterpiece\"\n",
      "                                                           sarcasmify: my white inheritance                                      report: all the other races coming to take your stuff                                                        \"Study: Everyone's just dying to get their hands on your privilege!\"\n",
      "                                     sarcasmify: 5 ways to file your taxes with less stress               woman going to take quick break after filling out name, address on tax forms                                                    \"5 Ingenious Techniques to Turn Your Tax Filing into a Relaxing Spa Day\"\n",
      "                                             sarcasmify: lots of parents know this scenario              new babysitter can already tell this kind of kid who gets naked for no reason                             \"parents everywhere have mastered the art of pretending their kids aren’t total chaos machines\"\n"
     ]
    }
   ],
   "source": [
    "print(df_results.head().to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.rename(columns={\"generated\": \"target\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_enhanced = Dataset.from_pandas(df_results).train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13486/13486 [00:01<00:00, 10685.77 examples/s]\n",
      "Map: 100%|██████████| 1499/1499 [00:00<00:00, 11089.31 examples/s]\n",
      "/Users/samuelthen/Documents/CS4248/cs4248-project/.venv/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/zr/tc7r4hpj6tscx9pf08qck6m80000gn/T/ipykernel_27705/1849639684.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5058' max='5058' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5058/5058 33:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.908300</td>\n",
       "      <td>0.816178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.775100</td>\n",
       "      <td>0.790832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.691500</td>\n",
       "      <td>0.793273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t5_model_enhanced, t5_tokenizer_enhanced = train_model(\"t5-base\", T5ForConditionalGeneration, T5Tokenizer, dataset_enhanced, dataset_name=\"enhanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13486/13486 [00:02<00:00, 6457.64 examples/s]\n",
      "Map: 100%|██████████| 1499/1499 [00:00<00:00, 5274.33 examples/s]\n",
      "/Users/samuelthen/Documents/CS4248/cs4248-project/.venv/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/zr/tc7r4hpj6tscx9pf08qck6m80000gn/T/ipykernel_27705/1849639684.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5058' max='5058' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5058/5058 22:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.018400</td>\n",
       "      <td>0.895928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.705800</td>\n",
       "      <td>0.845994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.457400</td>\n",
       "      <td>0.874207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelthen/Documents/CS4248/cs4248-project/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3353: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bart_model_enhanced, bart_tokenizer_enhanced = train_model(\"facebook/bart-base\", BartForConditionalGeneration, BartTokenizer, dataset_enhanced, dataset_name=\"enhanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13486/13486 [00:01<00:00, 10387.34 examples/s]\n",
      "Map: 100%|██████████| 1499/1499 [00:00<00:00, 10496.53 examples/s]\n",
      "/Users/samuelthen/Documents/CS4248/cs4248-project/.venv/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/zr/tc7r4hpj6tscx9pf08qck6m80000gn/T/ipykernel_27705/1849639684.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5058' max='5058' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5058/5058 36:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.917700</td>\n",
       "      <td>0.803381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.775300</td>\n",
       "      <td>0.777147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.696100</td>\n",
       "      <td>0.777703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flan_model_enhanced, flan_tokenizer_enhanced = train_model(\"google/flan-t5-base\", T5ForConditionalGeneration, T5Tokenizer, dataset_enhanced, dataset_name=\"enhanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `jaguar land rover pauses shipments to u.s. amid new tariffs`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `\"Jaguar land rover bravely decides to stop shipments to the U.S., because who doesn’t love a little chaos?\"` |\n",
       "| Bart      | `\"Jaguar Land rover pauses shipments to the U.S. because who doesn’t love a good trade war?\"` |\n",
       "| FLAN-T5   | `\"Jaguar land rover decides to stop shipments to the U.S., because who doesn’t love a little tariff drama?\"` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `president trump's tariffs trigger global market turmoil`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `\"President Trump's tariffs: because who doesn't love a little chaos in the global economy?\"` |\n",
       "| Bart      | `\"President Trump’s tariffs spark global chaos—because who doesn’t love a good chaos?\"` |\n",
       "| FLAN-T5   | `\"President Trump's tariffs: because who doesn't love a good trade war?\"` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `global reaction to trump's sweeping tariff measures`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `\"global reaction to trump's sweeping tariff measures: because who doesn't love a little chaos in their lives?\"` |\n",
       "| Bart      | `\"global reaction to trump's latest tariffs: because who doesn’t love a little chaos?\"` |\n",
       "| FLAN-T5   | `\"global reaction to trump's groundbreaking tariff measures: because who doesn't love a good surprise?\"` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `potential worldwide recession feared due to u.s. tariffs`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `\"Global economy poised for a thrilling rollercoaster ride thanks to U.S. tariffs\"` |\n",
       "| Bart      | `\"Experts predict global recession will be just a minor inconvenience thanks to U.S. tariffs\"` |\n",
       "| FLAN-T5   | `\"U.S. tariffs: because who doesn’t love a good economic meltdown?\"` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `vietnam, india, and israel in talks over new trade deals`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `\"Vietnam, India, and Israel engage in thrilling round of trade talks, because who doesn’t love a good trade war?\"` |\n",
       "| Bart      | `\"Vietnam, India, and Israel engage in deep intellectual discussions over trade deals, because who doesn't love a good game of geopolitical chicken?\"` |\n",
       "| FLAN-T5   | `\"vietnam, india, and israel engage in thrilling game of 'who can trade more?'\"` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `u.s. supreme court allows freeze on dei-focused teacher training grants`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `\"u.s. supreme court finally decides to freeze dei-focused teacher training grants, because why not?\"` |\n",
       "| Bart      | `\"u.s. supreme court decides to freeze dei-focused teacher training grants because who needs funding anyway?\"` |\n",
       "| FLAN-T5   | `\"u.s. supreme court decides freeze on dei-focused teacher training grants is just too mainstream\"` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `chevron ordered to pay $744 million for environmental damages`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `\"chevron generously offers $744 million for environmental damage, because who doesn't love a good environmental disaster?\"` |\n",
       "| Bart      | `\"Chevron generously offers to pay $744 million for environmental damages, because who doesn’t love a good game of financial dodgeball?\"` |\n",
       "| FLAN-T5   | `\"chevron receives $744 million for environmental damage, because who doesn’t love a good surprise?\"` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `surge in measles cases reported in texas`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `\"Texas residents thrilled to experience the thrill of measles in a state that definitely needs more of those.\"` |\n",
       "| Bart      | `\"Texas rolls out the red carpet for measles, because who doesn’t love a little extra excitement?\"` |\n",
       "| FLAN-T5   | `\"Texas' measles cases skyrocket, because who doesn't love a good snoozefest?\"` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `los angeles county settles $4 billion abuse case in juvenile facilities`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `\"Los Angeles County settles $4 billion abuse case, because who doesn’t love a good family drama?\"` |\n",
       "| Bart      | `\"Los Angeles County finally decides to pay kids a little more for their abuse—what a generous move!\"` |\n",
       "| FLAN-T5   | `\"Los Angeles County finally decides to settle a $4 billion abuse case—because who doesn’t love a good game of hide-and-seek?\"` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `anti-trump protests erupt across the u.s.`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `\"anti-trump protests erupt across the u.s., because who doesn’t love a little chaos?\"` |\n",
       "| Bart      | `\"Nationwide temper tantrum erupts as anti-Trump protests take a leisurely stroll across the U.S.\"` |\n",
       "| FLAN-T5   | `\"anti-trump protests take a leisurely stroll through the u.s., because who doesn’t love a good parade?\"` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `marsiling-yew tee to receive new community spaces and parks`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `\"Marsiling-Yew Tee: Because Who Needs Community Spaces When You Have Parks?\"` |\n",
       "| Bart      | `\" marsiling-yew tee to revolutionize community spaces and parks, because clearly that’s what the world needs right now\"` |\n",
       "| FLAN-T5   | `\"marsiling-yew tee thrilled to receive new community spaces and parks, because why not?\"` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `political parties debate balance of foreign talent in singapore`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `\"Political parties grapple with the delicate balance of foreign talent in Singapore—because who doesn’t love a good debate?\"` |\n",
       "| Bart      | `\"Political parties expertly navigate the thrilling world of foreign talent insingapore\"` |\n",
       "| FLAN-T5   | `\"political parties debate if foreign talent is just a trendy accessory in singapore\"` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `opel to cease vehicle sales in singapore by end-2025`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `\"opel announces groundbreaking plan to stop selling cars in singapore by the end of 2025\"` |\n",
       "| Bart      | `\"OPL announces grand plans to cease vehicle sales insingapore by end-2025, because who needs customers anyway?\"` |\n",
       "| FLAN-T5   | `\"Opel announces groundbreaking plan to stop selling vehicles in Singapore by the end of 2025—because who doesn’t love a good nostalgia trip?\"` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `singapore's economy projected to grow by 2.6% in 2025`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `\"Singapore's economy poised to grow by 2.6% in 2025—because who doesn't love a good economic forecast?\"` |\n",
       "| Bart      | `\"singapore's economy set to magically grow by 2.6% in 2025—because who needs growth anyway?\"` |\n",
       "| FLAN-T5   | `\"Singapore's economy predicted to grow by a staggering 2.6% in 2025—because who doesn't love a good surprise?\"` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 📰 Original: `temporary bus diversions for singapore t100 2025 event`  \n",
       "| Model | Output |\n",
       "|-------|--------|\n",
       "| T5        | `\"Singapore T100 2025: Because Who Doesn't Love a Good Bus Diversion?\"` |\n",
       "| Bart      | `\"singapore t100 2025 event: because who doesn’t want to relive the glory days of overpriced bus diversions?\"` |\n",
       "| FLAN-T5   | `\"Singapore T100 2025: Because Who Doesn't Love a Good Bus Diversion?\"` |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for h in test_headlines:\n",
    "    t5_out_enhanced = generate(t5_model_enhanced, t5_tokenizer_enhanced, h)\n",
    "    bart_out_enhanced = generate(bart_model_enhanced, bart_tokenizer_enhanced, h)\n",
    "    flan_out_enhanced = generate(flan_model_enhanced, flan_tokenizer_enhanced, h)\n",
    "\n",
    "    display(Markdown(f\"\"\"\n",
    "### 📰 Original: `{h}`  \n",
    "| Model | Output |\n",
    "|-------|--------|\n",
    "| T5        | `{t5_out_enhanced}` |\n",
    "| Bart      | `{bart_out_enhanced}` |\n",
    "| FLAN-T5   | `{flan_out_enhanced}` |\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
