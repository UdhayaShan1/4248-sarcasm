[nltk_data] Downloading package stopwords to[nltk_data]     /home/y/yuchenbo/nltk_data...[nltk_data]   Package stopwords is already up-to-date![nltk_data] Downloading package wordnet to[nltk_data]     /home/y/yuchenbo/nltk_data...[nltk_data]   Package wordnet is already up-to-date![nltk_data] Downloading package punkt to /home/y/yuchenbo/nltk_data...[nltk_data]   Package punkt is already up-to-date![nltk_data] Downloading package averaged_perceptron_tagger to[nltk_data]     /home/y/yuchenbo/nltk_data...[nltk_data]   Package averaged_perceptron_tagger is already up-to-[nltk_data]       date![nltk_data] Downloading package averaged_perceptron_tagger_eng to[nltk_data]     /home/y/yuchenbo/nltk_data...[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-[nltk_data]       date![nltk_data] Downloading package punkt_tab to[nltk_data]     /home/y/yuchenbo/nltk_data...[nltk_data]   Package punkt_tab is already up-to-date![nltk_data] Downloading package averaged_perceptron_tagger to[nltk_data]     /home/y/yuchenbo/nltk_data...[nltk_data]   Package averaged_perceptron_tagger is already up-to-[nltk_data]       date![nltk_data] Downloading package stopwords to[nltk_data]     /home/y/yuchenbo/nltk_data...[nltk_data]   Package stopwords is already up-to-date![nltk_data] Downloading package wordnet to[nltk_data]     /home/y/yuchenbo/nltk_data...[nltk_data]   Package wordnet is already up-to-date![nltk_data] Downloading package punkt to /home/y/yuchenbo/nltk_data...[nltk_data]   Package punkt is already up-to-date!2025-04-09 16:36:11.588326: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.  Processed batch 50/895  Processed batch 100/895  Processed batch 150/895  Processed batch 200/895  Processed batch 250/895  Processed batch 300/895  Processed batch 350/895  Processed batch 400/895  Processed batch 450/895  Processed batch 500/895  Processed batch 550/895  Processed batch 600/895  Processed batch 650/895  Processed batch 700/895  Processed batch 750/895  Processed batch 800/895  Processed batch 850/895(28619, 768)Extracting linguistic features...Linguistic features shape: (28619, 17)Combining BERT [CLS] and linguistic features...Combined BERT+Ling shape: (28619, 785)Splitting data into train/test sets...Train features shape: (21464, 785), Test features shape: (7155, 785)Scaling combined BERT+Ling features...Scaled features shape (sparse): (21464, 785)Generating TF-IDF features...TF-IDF Train shape: (21464, 23197)TF-IDF Test shape: (7155, 23197)Combining TF-IDF and Scaled (BERT+Ling) features...Final Combined Train shape: (21464, 23982)Final Combined Test shape: (7155, 23982)Training Logistic Regression model...Evaluating Logistic Regression model...Classification Report:              precision    recall  f1-score   support           0       0.89      0.89      0.89      3746           1       0.88      0.88      0.88      3409    accuracy                           0.89      7155   macro avg       0.89      0.89      0.89      7155weighted avg       0.89      0.89      0.89      7155Confusion Matrix:[[3347  399] [ 394 3015]]Macro F1 Score: 0.8889--- Script Finished ---%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%### Bert[batch_size, 64, 768] -> [CLS][batch_size, 768] -> DropOut -> Linear[2] {2 Epochs}Epoch 1/2At epoch  0Evaluation Results:              precision    recall  f1-score   support           0       0.89      0.94      0.91      3730           1       0.93      0.87      0.90      3425    accuracy                           0.91      7155   macro avg       0.91      0.90      0.91      7155weighted avg       0.91      0.91      0.91      7155Macro F1 score: 0.9053322768434772Epoch 2/2At epoch  1Evaluation Results:              precision    recall  f1-score   support           0       0.92      0.95      0.93      3730           1       0.94      0.90      0.92      3425    accuracy                           0.93      7155   macro avg       0.93      0.93      0.93      7155weighted avg       0.93      0.93      0.93      7155Macro F1 score: 0.9263537745186716