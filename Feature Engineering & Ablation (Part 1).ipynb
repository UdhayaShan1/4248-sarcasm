{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiaRJEN6J5FO"
      },
      "source": [
        "# **Sarcasm Classification: Feature Engineering & Ablation (Part 1)**\n",
        "\n",
        "**Author: Shanmugam Udhaya**\n",
        "\n",
        "*Contact:* [@frostbitepillars](https://t.me/frostbitepillars) for any clarifications  \n",
        "\n",
        "---\n",
        "\n",
        "This notebook is the first in a series exploring the task of sarcasm detection on a headline dataset.\n",
        "\n",
        "In this part, we focus on:\n",
        "\n",
        "- Possible pre-processing\n",
        "- Explore of linguistic features\n",
        "- Evaluate found features through ablation analysis\n",
        "\n",
        "---\n",
        "\n",
        "## **Baseline Model Reference**\n",
        "We setup a baseline model with\n",
        "- Pre-Processing with removal of stopwords with no stemming or lemmatization\n",
        "- Feature Engineering includes only `tf-idf` with unigrams only\n",
        "- Model is `LogisticRegression` from sklearn with class_weight='balanced' with `max_iter` increased as needed if convergence warning is raised.\n",
        "- `train_test_split` with `random_state=42` and `stratify=df['is_sarcastic']`\n",
        "- **0.797** Macro F1\n",
        "\n",
        "\n",
        "## **Summary of Findings**\n",
        "\n",
        "### Preprocessing:\n",
        "\n",
        "- **No text pre-processing done** (e.g., no stemming, no lowercasing, no punctuation removal)\n",
        "- Achieve increased score to **0.836**\n",
        "\n",
        "### Linguistic Features Used:\n",
        "- The following 17 handcrafted features were selected through ablation to capture structural, lexical, and stylistic cues:\n",
        "- This helps increase baseline score from **0.836 to 0.850**\n",
        "\n",
        "| Feature Name               | Description |\n",
        "|----------------------------|-------------|\n",
        "| `text_length`              | Number of tokens in the headline |\n",
        "| `noun_count`               | Number of nouns |\n",
        "| `verb_count`               | Number of verbs |\n",
        "| `adj_count`                | Number of adjectives |\n",
        "| `adv_count`                | Number of adverbs |\n",
        "| `dale_chall_score`         | Readability score |\n",
        "| `sentiment_score`          | Compound sentiment polarity |\n",
        "| `char_count`               | Total number of characters |\n",
        "| `capital_char_count`       | Count of capitalized characters |\n",
        "| `capital_word_count`       | Number of words in all caps |\n",
        "| `stopword_count`           | Number of stopwords |\n",
        "| `stopwords_vs_words`       | Ratio of stopwords to total words |\n",
        "| `contrastive_marker`       | Presence of words like \"but\", \"however\", etc. |\n",
        "| `entropy`                  | Lexical entropy (word distribution randomness) |\n",
        "| `lexical_diversity`        | Unique words divided by total words |\n",
        "| `sentiment_incongruity`    | Difference between overall sentiment and word-level polarity |\n",
        "| `difficult_word_count`     | Number of complex/difficult words |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPQ52drUTFHv",
        "outputId": "99b46af2-c7e2-4b28-b910-0e24c402a5c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsYRbMbCl3bA",
        "outputId": "74605456-8052-462e-a91d-01e71ebb2937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1eVjlFhPvpgXmj-wH52qAzdIUyHrfs92G\n",
            "To: /content/archive (7).zip\n",
            "\r  0% 0.00/3.46M [00:00<?, ?B/s]\r100% 3.46M/3.46M [00:00<00:00, 70.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1eVjlFhPvpgXmj-wH52qAzdIUyHrfs92G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTPnvgBZl8OE",
        "outputId": "4d665b21-3536-4ace-919d-c4ff7951c3d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  archive (7).zip\n",
            "  inflating: Sarcasm_Headlines_Dataset.json  \n",
            "  inflating: Sarcasm_Headlines_Dataset_v2.json  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"archive (7).zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xw8ttrdEmn92"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKotEXcY-Ape",
        "outputId": "e8dc7f16-3b05-422a-8c13-1f2fa873dce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['is_sarcastic', 'headline', 'article_link'], dtype='object')\n",
            "is_sarcastic    0\n",
            "headline        0\n",
            "article_link    0\n",
            "dtype: int64\n",
            "Counter({0: 14985, 1: 13634})\n",
            "   is_sarcastic                                           headline  \\\n",
            "0             1  thirtysomething scientists unveil doomsday clo...   \n",
            "1             0  dem rep. totally nails why congress is falling...   \n",
            "2             0  eat your veggies: 9 deliciously different recipes   \n",
            "3             1  inclement weather prevents liar from getting t...   \n",
            "4             1  mother comes pretty close to using word 'strea...   \n",
            "5             0                               my white inheritance   \n",
            "6             0         5 ways to file your taxes with less stress   \n",
            "7             1  richard branson's global-warming donation near...   \n",
            "8             1  shadow government getting too large to meet in...   \n",
            "9             0                 lots of parents know this scenario   \n",
            "\n",
            "                                        article_link  \n",
            "0  https://www.theonion.com/thirtysomething-scien...  \n",
            "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
            "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
            "3  https://local.theonion.com/inclement-weather-p...  \n",
            "4  https://www.theonion.com/mother-comes-pretty-c...  \n",
            "5  https://www.huffingtonpost.com/entry/my-white-...  \n",
            "6  https://www.huffingtonpost.com/entry/5-ways-to...  \n",
            "7  https://www.theonion.com/richard-bransons-glob...  \n",
            "8  https://politics.theonion.com/shadow-governmen...  \n",
            "9  https://www.huffingtonpost.comhttp://pubx.co/6...  \n"
          ]
        }
      ],
      "source": [
        "print(df.columns)\n",
        "print(df.isnull().sum())\n",
        "print(Counter(df['is_sarcastic']))\n",
        "print(df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLAfJQ3W-b4E"
      },
      "source": [
        "## Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGttj2KWdMsb"
      },
      "source": [
        "### No Pre-Processing at all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tfKLOEeKdJeg"
      },
      "outputs": [],
      "source": [
        "df['clean_headline'] = df['headline']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDhJf42edPtb"
      },
      "source": [
        "### With Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wsaM8QL_-B56"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text, action, stopword):\n",
        "  #Lower Caps\n",
        "  #text = text.lower()\n",
        "  #Remove Punctuations\n",
        "  #text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  #https://www.geeksforgeeks.org/text-preprocessing-for-nlp-tasks/\n",
        "    # text = text.lower()  # Lowercase\n",
        "  #text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    #text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "  #text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    # text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
        "  # Tokenize and remove stopwords\n",
        "  words = word_tokenize(text)\n",
        "  if stopword:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "  #If stemming\n",
        "  if action == \"S\":\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "  elif action == \"L\":\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "  return \" \".join(words)\n",
        "\n",
        "# Apply preprocessing to the text column\n",
        "df['clean_headline'] = df['headline'].apply(lambda text: preprocess_text(text, \"\", False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5Qx2QOvQhUu"
      },
      "source": [
        "## Download Glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmzeQFRDM2C4",
        "outputId": "26d5a94e-2197-4169-cc5f-6385128db608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1HPpXpaVGK1G6W1G5wPZwF9p091qacR8i\n",
            "From (redirected): https://drive.google.com/uc?id=1HPpXpaVGK1G6W1G5wPZwF9p091qacR8i&confirm=t&uuid=9efc0b3b-99ac-446e-9cac-27a33941f0ca\n",
            "To: /content/glove.6B.zip\n",
            "100% 862M/862M [00:24<00:00, 35.7MB/s]\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!gdown 1HPpXpaVGK1G6W1G5wPZwF9p091qacR8i\n",
        "!unzip \"glove.6B.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bpt_1L25KrtF",
        "outputId": "3fa4895c-8364-44e5-b7d9-3ab1ea1a1f32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1.4999e-01  5.3597e-02  9.4669e-02  1.2415e-01 -1.0623e-01  3.2981e-01\n",
            " -3.6563e-02 -4.9109e-01  5.0600e-02 -4.8218e-01  6.9264e-01 -1.5298e-01\n",
            " -2.3069e-01  8.3252e-02  5.6969e-02 -4.4769e-01  2.7878e-01  7.0629e-02\n",
            " -2.8340e-01  4.1989e-01  3.3607e-01  3.3273e-01 -4.2430e-01  1.3433e-01\n",
            "  2.4444e-01  3.6712e-01 -4.7969e-01 -3.8191e-01  1.8654e-01 -1.9120e-01\n",
            " -1.7775e-01 -2.2396e-01 -1.2442e+00 -4.2139e-01 -1.2342e+00  4.5623e-01\n",
            "  1.9550e-02  7.4867e-01  4.7384e-02 -7.7133e-02 -2.6682e-01 -3.6488e-01\n",
            " -2.4977e-02 -6.0338e-02  4.1059e-02  4.3062e-01  2.4870e-01  3.4548e-02\n",
            "  6.1338e-01 -4.3779e-02 -5.3384e-02  4.8766e-01 -4.4736e-02  9.4678e-02\n",
            " -2.7967e-01  7.3181e-01  5.5861e-01  8.9743e-02 -1.2702e-01 -4.8329e-02\n",
            "  1.3241e-01 -2.1868e-01  4.7130e-01  2.3780e-01 -1.1905e-01  1.4091e-01\n",
            "  3.4236e-02  5.8102e-02 -1.0685e-01 -1.2360e-01 -6.4432e-01 -1.2913e-02\n",
            "  5.6400e-02  4.5082e-01 -1.1311e-01 -2.9463e-01 -4.4107e-02 -1.0306e-01\n",
            "  5.9227e-02  8.7667e-02 -6.0326e-01 -1.5421e-01  4.7466e-01  5.4349e-02\n",
            "  9.4245e-02  6.7547e-01  9.3743e-02  3.9292e-02  2.1902e-01 -2.2840e-01\n",
            " -1.1696e-01 -1.9528e-01  5.8218e-02 -3.8343e-01  2.4897e-01 -2.5120e-01\n",
            " -5.1597e-01  4.2212e-02  1.6381e-01 -1.5598e-01  1.7026e-01  4.0956e-01\n",
            "  1.1879e-01 -1.7408e-01  3.9237e-01 -4.9366e-01  1.7514e-01  1.2423e-01\n",
            "  5.7063e-02  2.0705e-01  2.3778e-01  3.2887e-01  5.5798e-02 -1.9037e-01\n",
            "  2.5897e-01 -3.3326e-01  1.4965e-01  2.3986e-01 -1.3822e-01  2.7421e-01\n",
            "  1.7235e-01 -2.9496e-01  2.7371e-03  8.5009e-02  2.4704e-01 -2.4956e-01\n",
            " -1.1558e-02  5.6495e-01 -3.7383e-01  2.8987e-01 -2.1087e-01 -5.7253e-02\n",
            " -1.2836e-01  9.3511e-02 -7.9462e-01  2.0676e-02 -1.8020e-01 -3.4054e-04\n",
            " -2.5556e-01  7.0471e-01 -4.2855e-01  7.6964e-02 -2.3587e-01  1.0920e-01\n",
            "  2.3406e-02  1.3370e-01 -5.9503e-01  1.5626e-01 -2.1945e-01  4.8405e-02\n",
            " -6.2798e-01 -4.6496e-01 -1.1485e-01 -7.3404e-01  6.0442e-01 -2.2954e-01\n",
            " -1.5352e-01  2.3170e-01 -1.9323e-01 -3.4891e-01  1.0769e-01 -3.2297e-01\n",
            "  4.4082e-01  1.6618e-01 -3.9881e-01  2.0095e-01 -2.7796e-01  1.5464e-01\n",
            " -1.0777e-01 -2.4873e-02  1.4706e-01 -5.2247e-01 -3.5819e-01 -4.4224e-02\n",
            " -4.7287e-02  1.7179e-01 -3.2419e-01 -1.5742e-01  1.0100e-02  8.4755e-02\n",
            " -2.7929e-01 -4.3222e-01  7.0830e-01 -2.1706e-01  1.6192e-01  5.8348e-03\n",
            "  4.3244e-01 -2.1973e-01 -4.5149e-02  6.0086e-01 -3.4385e-01  5.2345e-02\n",
            "  3.0757e-01  8.8022e-02 -2.9415e-01  3.1277e-03 -4.7851e-01  2.1288e-01\n",
            " -4.9584e-01 -1.0055e-01  1.4569e+00  4.2731e-02 -6.8303e-02  1.4197e-01\n",
            " -3.4525e-01  2.7788e-01 -2.0811e-02  4.4559e-01 -1.9598e-01 -4.4542e-01\n",
            " -5.8920e-01 -1.6964e-01  1.0168e-01 -6.2910e-01  3.4323e-01 -3.0170e-01\n",
            " -5.3384e-01 -3.9779e-02  1.8163e-02 -4.8881e-02 -2.0804e-01 -1.9576e-01\n",
            " -3.1244e-01  7.4901e-02 -3.2654e-02 -1.2901e-01  2.1407e-01 -1.9287e-01\n",
            " -3.2234e-01 -1.3269e-01 -4.0424e-01  6.5655e-02 -1.0051e-01 -8.6236e-01\n",
            " -2.8633e-01  3.5023e-01  1.0131e-01 -1.3971e-01 -2.7010e-01 -9.5299e-03\n",
            "  6.2880e-01 -5.4393e-02 -5.5348e-01  7.5561e-01 -3.2622e-01 -3.9326e-02\n",
            " -3.7252e-01  2.7311e-01  2.2658e-01 -1.5553e-01  2.3109e-01 -7.8274e-01\n",
            " -2.1581e-01 -4.1757e-01  3.3484e-01 -8.8353e-02 -8.5984e-02 -2.1289e-01\n",
            " -3.7182e-01 -3.1643e-01  2.1288e-01 -3.6237e-02 -9.2752e-02  6.6673e-03\n",
            "  2.4336e-01  2.0090e-01 -3.9357e-01 -8.8579e-02  4.3742e-02 -1.2053e-01\n",
            " -2.6795e-01 -2.5263e-01  5.1712e-02 -5.7308e-02  3.7963e-01  6.7176e-02\n",
            " -7.4044e-01  5.1321e-02 -9.9559e-02 -1.1743e-01 -6.3087e-02  2.9985e-02\n",
            "  1.8439e-01  5.3850e-03 -1.4725e-01 -4.1222e-01 -6.7534e-01  3.5182e-01\n",
            " -1.1665e-01  1.0121e-01 -2.8311e-01  4.4419e-03 -5.0686e-01  3.2047e-01\n",
            " -3.5880e-03 -2.8969e-01  8.2759e-02  3.0646e-01  6.6816e-02 -2.5007e-01]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_glove_embeddings(glove_path):\n",
        "    embeddings = {}\n",
        "    with open(glove_path, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            word = parts[0]\n",
        "            vector = np.array(parts[1:], dtype=np.float32)\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "glove_path = \"glove.6B.300d.txt\"  # adjust path if needed\n",
        "glove_dict = load_glove_embeddings(glove_path)\n",
        "\n",
        "# Example: get vector for a word\n",
        "print(glove_dict['amazing'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRqdYVXpKwTW"
      },
      "outputs": [],
      "source": [
        "def sentence_to_glove_vector(sentence, glove_dict, dim=300):\n",
        "    words = sentence.lower().split()\n",
        "    vectors = [glove_dict[word] for word in words if word in glove_dict]\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(dim)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "df['word_embedding'] = df['clean_headline'].apply(\n",
        "    lambda x: sentence_to_glove_vector(x, glove_dict, dim=300)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-_FmBFM_R-V"
      },
      "source": [
        "## tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDH1nNzw-971",
        "outputId": "805570f1-0cdf-4b36-d390-8639f795ef03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(21464, 23183) (7155, 23183)\n"
          ]
        }
      ],
      "source": [
        "tf_idf = TfidfVectorizer()\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(df['clean_headline'], df['is_sarcastic'], random_state=42, stratify=df['is_sarcastic'])\n",
        "\n",
        "X_train_idf = tf_idf.fit_transform(X_train)\n",
        "X_test_idf = tf_idf.transform(X_test)\n",
        "print(X_train_idf.shape, X_test_idf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhK1Ayq7VKTp",
        "outputId": "faaa2f0c-c52b-4cf9-be54-783f3ae24b6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5236209467014536\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "count_y_train = Counter(Y_train)\n",
        "total = sum(count_y_train.values())\n",
        "print(count_y_train[0] / total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzLn49LYYikV",
        "outputId": "7e1030a2-5ea3-4333-9263-4ba4fab70469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({0: 14985, 1: 13634})\n",
            "0.5236209467014536\n"
          ]
        }
      ],
      "source": [
        "print(Counter(df['is_sarcastic']))\n",
        "count_is_sarcastic = Counter(Y_train)\n",
        "total_y = sum(count_is_sarcastic.values())\n",
        "print(count_is_sarcastic[0] / total_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqa5ewEBrink"
      },
      "source": [
        "## Linguistic Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PSn-tKoxbK0"
      },
      "source": [
        "### POS features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "N845Yq8QriMl"
      },
      "outputs": [],
      "source": [
        "def get_pos_counts(text):\n",
        "    \"\"\"\n",
        "    Returns a dictionary with counts of certain POS tags (NOUN, VERB, ADJ, ADV)\n",
        "    \"\"\"\n",
        "    pos_tags = pos_tag(word_tokenize(text))\n",
        "    counts = {\n",
        "        'noun_count': 0,\n",
        "        'verb_count': 0,\n",
        "        'adj_count': 0,\n",
        "        'adv_count': 0\n",
        "    }\n",
        "    for word, tag in pos_tags:\n",
        "        if tag.startswith('NN'):\n",
        "            counts['noun_count'] += 1\n",
        "        elif tag.startswith('VB'):\n",
        "            counts['verb_count'] += 1\n",
        "        elif tag.startswith('JJ'):\n",
        "            counts['adj_count'] += 1\n",
        "        elif tag.startswith('RB'):\n",
        "            counts['adv_count'] += 1\n",
        "    return counts\n",
        "\n",
        "def get_text_length(text):\n",
        "    return len(word_tokenize(text))\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_ner_count(text):\n",
        "    doc = nlp(text)\n",
        "    return len(doc.ents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txzV71JJs6L3",
        "outputId": "d9f0b3ac-4e88-4cd8-b3b9-d0908ec8bb0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                      clean_headline  noun_count  verb_count  \\\n",
            "0  thirtysomething scientists unveil doomsday clo...           4           1   \n",
            "1  dem rep. totally nails why congress is falling...           5           2   \n",
            "2  eat your veggies : 9 deliciously different rec...           2           1   \n",
            "3  inclement weather prevents liar from getting t...           3           3   \n",
            "4  mother comes pretty close to using word 'strea...           2           3   \n",
            "\n",
            "   adj_count  adv_count  text_length  ner_count  \n",
            "0          2          0            8          1  \n",
            "1          3          1           14          2  \n",
            "2          1          1            8          1  \n",
            "3          0          0            8          0  \n",
            "4          1          2           10          0  \n"
          ]
        }
      ],
      "source": [
        "df['pos_counts'] = df['clean_headline'].apply(get_pos_counts)\n",
        "df['text_length'] = df['clean_headline'].apply(get_text_length)\n",
        "df['ner_count'] = df['clean_headline'].apply(get_ner_count)\n",
        "\n",
        "df['noun_count'] = df['pos_counts'].apply(lambda x: x['noun_count'])\n",
        "df['verb_count'] = df['pos_counts'].apply(lambda x: x['verb_count'])\n",
        "df['adj_count'] = df['pos_counts'].apply(lambda x: x['adj_count'])\n",
        "df['adv_count'] = df['pos_counts'].apply(lambda x: x['adv_count'])\n",
        "\n",
        "print(df[['clean_headline', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'text_length', 'ner_count']].head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pgO1WUNEY0i1"
      },
      "outputs": [],
      "source": [
        "df['noun_count'] = df['pos_counts'].apply(lambda x: x['noun_count'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya6D93bCxeh9"
      },
      "source": [
        "### Readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxv6H1LuxZAH",
        "outputId": "b7fd99df-ced2-4407-fbaf-8c28a25a2de4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting cmudict (from textstat)\n",
            "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.6.1)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.21.0)\n",
            "Downloading textstat-0.7.5-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, cmudict, textstat\n",
            "Successfully installed cmudict-1.0.32 pyphen-0.17.2 textstat-0.7.5\n"
          ]
        }
      ],
      "source": [
        "!pip install textstat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yxHndzybxYAc"
      },
      "outputs": [],
      "source": [
        "import textstat\n",
        "df['flesch_reading_ease'] = df['clean_headline'].apply(lambda text: textstat.flesch_reading_ease(text))\n",
        "df['dale_chall_score'] = df['clean_headline'].apply(lambda text: textstat.dale_chall_readability_score(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D5q85yn3D57"
      },
      "source": [
        "### Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZecEggrb3G97",
        "outputId": "beff9f61-b204-40d2-a248-14e51e69d01c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.1.31)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m112.6/126.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ULHVACtS3FgK"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "df['sentiment_score'] = df['clean_headline'].apply(lambda text: analyzer.polarity_scores(text)['compound'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDI30MuHqRvN"
      },
      "source": [
        "### Incongruity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8wsXxmiEqV1w"
      },
      "outputs": [],
      "source": [
        "def detect_incongruity(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_words = 0\n",
        "    neg_words = 0\n",
        "\n",
        "    for word in tokens:\n",
        "        score = analyzer.polarity_scores(word)['compound']\n",
        "        if score >= 0.5:\n",
        "            pos_words += 1\n",
        "        elif score <= -0.5:\n",
        "            neg_words += 1\n",
        "\n",
        "    # Return 1 if both positive and negative words exist → sentiment conflict\n",
        "    return int(pos_words > 0 and neg_words > 0)\n",
        "\n",
        "# Apply to the DataFrame\n",
        "df['sentiment_incongruity'] = df['clean_headline'].apply(detect_incongruity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM1NHGY93j7U"
      },
      "source": [
        "### Emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBq4YwoU3neS",
        "outputId": "3131d22c-79b1-4a5b-ec66-d296cd606156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting text2emotion\n",
            "  Downloading text2emotion-0.0.5-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from text2emotion) (3.9.1)\n",
            "Collecting emoji>=0.6.0 (from text2emotion)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->text2emotion) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->text2emotion) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->text2emotion) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->text2emotion) (4.67.1)\n",
            "Downloading text2emotion-0.0.5-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji, text2emotion\n",
            "Successfully installed emoji-2.14.1 text2emotion-0.0.5\n",
            "Collecting emoji==1.6.3\n",
            "  Downloading emoji-1.6.3.tar.gz (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.2/174.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170282 sha256=7ac6b1e2925c4c9014810b2ec372c1d19000fd1604c45a603d15a397352c953a\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/e1/6e/02053f229e270406b51792d6d511a55338c818642599fa9cfe\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "  Attempting uninstall: emoji\n",
            "    Found existing installation: emoji 2.14.1\n",
            "    Uninstalling emoji-2.14.1:\n",
            "      Successfully uninstalled emoji-2.14.1\n",
            "Successfully installed emoji-1.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install text2emotion\n",
        "!pip install emoji==1.6.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDsLr3vc675P"
      },
      "source": [
        "### Text Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bPrXkKPl6-4n"
      },
      "outputs": [],
      "source": [
        "def count_chars(text):\n",
        "    return len(text)\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def count_capital_chars(text):\n",
        "  count=0\n",
        "  for i in text:\n",
        "    if i.isupper():\n",
        "      count+=1\n",
        "  return count\n",
        "\n",
        "def count_capital_words(text):\n",
        "    return sum(map(str.isupper,text.split()))\n",
        "\n",
        "def count_unique_words(text):\n",
        "    return len(set(text.split()))\n",
        "\n",
        "def count_exclamation(text):\n",
        "    return text.count(\"!\")\n",
        "\n",
        "df['exclamation_count'] = df['clean_headline'].apply(count_exclamation)\n",
        "df['char_count'] = df['clean_headline'].apply(count_chars)\n",
        "df['word_count'] = df['clean_headline'].apply(count_words)\n",
        "df['capital_char_count'] = df[\"clean_headline\"].apply(lambda x:count_capital_chars(x))\n",
        "df['capital_word_count'] = df[\"clean_headline\"].apply(lambda x:count_capital_words(x))\n",
        "df['stopword_count'] = df['clean_headline'].apply(lambda x: len([word for word in x.split() if word in stopwords.words('english')]))\n",
        "\n",
        "\n",
        "df['avg_wordlength'] = df['char_count']/df['word_count']\n",
        "df['stopwords_vs_words'] = df['stopword_count']/df['word_count']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2ZJAsKpJBzy"
      },
      "source": [
        "### Irony Markers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BIyfe3-MJDe_"
      },
      "outputs": [],
      "source": [
        "def has_contrastive_conjunction(text):\n",
        "    contrastive_words = {\"but\", \"although\", \"yet\", \"however\", \"though\"}\n",
        "    return int(any(word in text.split() for word in contrastive_words))\n",
        "\n",
        "df['contrastive_marker'] = df['clean_headline'].apply(has_contrastive_conjunction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy-3f-lEJU5f"
      },
      "source": [
        "### Contextual Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490,
          "referenced_widgets": [
            "0809ac74dba9455ca12547fefbe282a0",
            "9292b75478d04a31b25540949ad4299b",
            "f81e79bf65c6449a8e030edf1d5b419f",
            "3cab9bc75ccc418e8e8506d39773ea1a",
            "e83b3b9fb4e54c04b6b65a5fd4eec31b",
            "7ec9a6b9a8504acf967748f0948171a2",
            "6b5907bf4dd74152b612d347f66fa870",
            "10f4547658e348008e43e481166544f5",
            "35a9725d889749b695f021d291918004",
            "c91152b6ef6647889164a06168e8027c",
            "c9e9f9b5a2394fb4abb8f1d314fb94a6",
            "714402c56b2c46cba6b4b41c1c354dba",
            "851a0d232903498bb9bc278e5ee426c7",
            "be2643c91d4a404486c79f18f833d039",
            "2b2befebc7904f1293ae243ea26e1b8f",
            "faecab59d9e0425f8efd184e5377a217",
            "1de6971426ad40e499cc52dc3593b403",
            "459159d770be4bbfbb811fa316cadf92",
            "1393cc7cc3a444e4b63bf977cc45b78c",
            "798e3dcd06ec4f8d89dba3ab952905ac",
            "4283fefe341b40abbc5c71f4c7e762f7",
            "0e92d5228dd04809ace1174f9d68dc35",
            "42bdcc222aee47689f414e6bc495a008",
            "99fb63df2d6b4141bb156bed50fb0d85",
            "7faa5041359843d998a548903dc2dd36",
            "26490dbf185e4b5289b0f9b62a3fbeb9",
            "801a53aac3914c9497392f3964d31b27",
            "308596e9270b4705b23ca1b31c6567e8",
            "7e5c6c3fbdfc4ac19e396e61f949529d",
            "1bdd0a4350274e42b799c262b8e68388",
            "a82061c9c42e4e5ca5cb82b314f3e874",
            "6676efc1d4d04e0f9f9ace7630ef6b25",
            "17c6dcfd470d48b9b2cc3352ac26bb6e",
            "41f36c7e8a7d4d0ba8f862675bf21f65",
            "88a11b40591448b8974c9e4afe84319a",
            "2a4aded30526450a98bde1cea71e6119",
            "e3fe76a054d349a69e8272ce0cc9a8ba",
            "e2df7ed414c04c47a5551ac570953fb5",
            "a088c92ad5bf42aa9f9b35a595eaf1ba",
            "bb3ec9d73e0f43ecb63dfbdf36bd859a",
            "20e140b95de747e1aa281d736e5a20e9",
            "ea72d1bb1501454aa87d6a4132e7c894",
            "fc656026835d4e83aa45ee239621e0a3",
            "1ae8eeb7c7e447be9ed7bc864696798f",
            "b8b07c1d17974f2cb72f32f61e60207f",
            "daafdefaafc5488d8bce5ef26e28e314",
            "7aa7954f3961475f857205718c126992",
            "d05b2a52751f44dfb7e6e599f5d6255e",
            "d6da3e7b17034a6f9341bead75672e37",
            "dd405b841ff946219e9b62842447cf3a",
            "6a90b304f9eb4395aad35f71e1653ea4",
            "1a2d8be0a1b9409ab9e7099d1efc3ae5",
            "88117b4f2df1422ab175ceefd45e3bd6",
            "48f2fdee53fe49d09a2ee29c9921f5dc",
            "6621ef3bc3524c48841f744bfdf6a6ce",
            "0217d3b05b0949728427e18341e28957",
            "b63341ca83fd40cea9059c8119321571",
            "4baf962d119a4b7dae6e9650578f6d59",
            "570997b5639a4ca4bd88cdf7be31b320",
            "4788e91c11dc43aeba2ac5cddf002b75",
            "6582531ec5264f5484db1ede7a9ace0e",
            "3cd59d9148734f6584a737c8ee25ab6d",
            "403f68efee7a4d6caa338b315de42051",
            "a4800dc3f9104d9ab5f24e0e1c51f4ce",
            "10b6503394af4b3d98c8d096c41caddf",
            "ba1daba181834f35af0aa978acad1723",
            "25a02e49a0d845f39fe02fa785f660e7",
            "323d64253c074649b2def555577afa0d",
            "566e44effd944905a3b41b32e9346363",
            "1f7f833df2f64774857461a75bdbd068",
            "a140657a733b4c65a9be15baae147c03",
            "b7299fce1017487994b43c7825fe4705",
            "9a78aeaad845433e89689465521df375",
            "9943c35925974aad91d40f624ed5cb5a",
            "3636a5b157c34c91bcaf51aa59756cc4",
            "f85a6452e0f04726a42b2e792466cd75",
            "de0b441cc1e046b998cf95bdef6eda62",
            "9ecdc39cbe9a4a27add1e50ce6bb88c5",
            "047319140c9448b293f105526c6154ed",
            "124a0a828e0a41559f1e3b4647d0514b",
            "b994af85d71947a18d265b64803d6301",
            "e4996505a87645a587018dfcc5d0a7ee",
            "eed819c4b54e4a8184d190eb73a6b737",
            "1b91a14cb9d84188a40cb09d4df15cd7",
            "e344b24f7f96415a9274b1060c28fe25",
            "8d948d23fd5249ceb534fbdd9ead4746",
            "714fcf426ebb4d21ab8d8b622b5f7e9a",
            "a95a3e1e8fce442ab65d14accaf826ad",
            "fd90a1b062fa46a2ba49a3bddbf1f88f",
            "596ff0f1e0634c98b7a9c1d98ed09ece",
            "ec06ba1a9eed48b29e2e684c017eb9fa",
            "050b6133b8594ee5bd9de4a351243838",
            "6b8521b4fa694bc49a483ffa2ab79a2a",
            "f1c3ba0ea94e4d76afcbe0a8d9f6ccd6",
            "abe957190b774fbb874f0395af76dac8",
            "b3734ec9a4f24aee832d23b3f1b87959",
            "2ddaf88c94e14b488eece83f4ff339da",
            "328220509f4a4193832842da0b6bc262",
            "8b7afcadb5884bb2badcda2549d30515",
            "8ea0aa2b50ec43f590758f3dc20a1f18",
            "db1fd5c6a21a4052bf8acb38125a5216",
            "01566cbb04d74ec1a3335529f5bfe94d",
            "663fce3b35d04acc96411918b42d144d",
            "7da3ccaa7b2341be95dbf32e1e4bd1ac",
            "94d26574690d4d8999590e76742ecdde",
            "ad34f6a788ca4a43a4e04c662d6ebc67",
            "a1f8fbb4dacf4938b3ecddf175b874c2",
            "23a6492d01d34c67a36a24651555b4f2",
            "2626a61037564558826d0b62ef17f0ec",
            "dda7d17bd7204220ae89b012ddd5e067",
            "3db0c8a687614019919c3b9b3fb1211c",
            "38ae3906c6e3448fb3ca21925e15a25f",
            "af268f2595b8402db399d7e2c8cc632b",
            "be19b2ea66b448f893ffc6301f7c3863",
            "0332638ffeda4b99a39c7d8b44479f2e",
            "048d3a7a288b49ed882bde8439846d67",
            "cb5bba2cdc864c088bc3cab4c6800452",
            "a8b3bad27fef4691b08ec3c70716959e",
            "f020dedabaf143b5ae7c595b2ef28762",
            "f9d22264e420439c8cfb22c5929e6404",
            "c8e45661e8e0455382d6a9e97017a8f1"
          ]
        },
        "id": "LCl63q-eJWnu",
        "outputId": "c074d145-9cee-4686-e9dc-097cc7dfbc2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0809ac74dba9455ca12547fefbe282a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "714402c56b2c46cba6b4b41c1c354dba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42bdcc222aee47689f414e6bc495a008",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41f36c7e8a7d4d0ba8f862675bf21f65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8b07c1d17974f2cb72f32f61e60207f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0217d3b05b0949728427e18341e28957",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25a02e49a0d845f39fe02fa785f660e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ecdc39cbe9a4a27add1e50ce6bb88c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd90a1b062fa46a2ba49a3bddbf1f88f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ea0aa2b50ec43f590758f3dc20a1f18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3db0c8a687614019919c3b9b3fb1211c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = SentenceTransformer('all-mpnet-base-v2', device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define neutral statement embedding once (avoid redundant computation)\n",
        "neutral_statement = \"This is a neutral news headline.\"\n",
        "neutral_embedding = model.encode([neutral_statement], convert_to_tensor=True).to(device)\n",
        "\n",
        "# Encode all headlines in batches (efficient batch processing with GPU)\n",
        "headlines = df['clean_headline'].tolist()\n",
        "headline_embeddings = model.encode(headlines, batch_size=128, convert_to_tensor=True, device=device)\n",
        "# Compute cosine similarity using GPU tensors\n",
        "neutral_embedding = neutral_embedding / neutral_embedding.norm(dim=-1, keepdim=True)\n",
        "headline_embeddings = headline_embeddings / headline_embeddings.norm(dim=-1, keepdim=True)\n",
        "\n",
        "cosine_similarities = (headline_embeddings @ neutral_embedding.T).cpu().numpy().flatten()\n",
        "\n",
        "# Store results\n",
        "df['contextual_similarity'] = cosine_similarities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gik14uB7PUng"
      },
      "source": [
        "### Syntactic Complexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWixFqZmPZSd"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_syntactic_complexity(text):\n",
        "    doc = nlp(text)\n",
        "    depth = max([len(list(token.ancestors)) for token in doc])  # Sentence depth\n",
        "    clause_count = sum(1 for token in doc if token.dep_ in [\"ccomp\", \"advcl\", \"acl\"])\n",
        "    mean_dep_length = sum(abs(token.head.i - token.i) for token in doc) / len(doc)\n",
        "\n",
        "    return depth, clause_count, mean_dep_length\n",
        "\n",
        "df[['sentence_depth', 'clause_count', 'mean_dep_length']] = df['clean_headline'].apply(\n",
        "    lambda x: pd.Series(get_syntactic_complexity(x))\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRaXx9rnR1Si"
      },
      "source": [
        "### https://www.sciencedirect.com/science/article/pii/S1877050924007762"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYbw39fZSasQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import textstat\n",
        "import string\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from textblob import TextBlob\n",
        "from scipy.stats import entropy\n",
        "\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYPJRGzAR0yb"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import entropy\n",
        "def calculate_entropy(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    freq_dist = Counter(words)\n",
        "    probs = np.array(list(freq_dist.values())) / sum(freq_dist.values())\n",
        "    return entropy(probs, base=2)  # Shannon Entropy\n",
        "\n",
        "df[\"entropy\"] = df[\"clean_headline\"].apply(calculate_entropy)\n",
        "\n",
        "### 2. **Lexical Diversity (Unique Words / Total Words)**\n",
        "def lexical_diversity(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    return len(set(words)) / len(words) if len(words) > 0 else 0\n",
        "\n",
        "df[\"lexical_diversity\"] = df[\"clean_headline\"].apply(lexical_diversity)\n",
        "\n",
        "### 6. **Wrong Words (Words Not in WordNet)**\n",
        "def count_wrong_words(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    return sum(1 for word in words if not wordnet.synsets(word))\n",
        "\n",
        "df[\"wrong_word_count\"] = df[\"clean_headline\"].apply(count_wrong_words)\n",
        "\n",
        "### 7. **Difficult Words (Hard-to-Read Words)**\n",
        "df[\"difficult_word_count\"] = df[\"clean_headline\"].apply(textstat.difficult_words)\n",
        "\n",
        "### 8. **Lengthy Words (Words > 2 Characters)**\n",
        "df[\"lengthy_word_count\"] = df[\"clean_headline\"].apply(lambda words: sum(1 for word in words if len(word) > 2))\n",
        "\n",
        "### 9. **Two-Letter Words**\n",
        "df[\"two_letter_words\"] = df[\"clean_headline\"].apply(lambda words: sum(1 for word in words if len(word) == 2))\n",
        "\n",
        "### 10. **Single-Letter Words**\n",
        "df[\"single_letter_words\"] = df[\"clean_headline\"].apply(lambda words: sum(1 for word in words if len(word) == 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-gO78LwAV6FL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import textstat\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from scipy.stats import entropy\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example dataset\n",
        "# df = pd.read_csv(\"sarcasm_dataset.csv\")\n",
        "\n",
        "# Feature extraction functions\n",
        "def lexical_diversity(text):\n",
        "    words = word_tokenize(text)\n",
        "    return len(set(words)) / max(len(words), 1)  # Avoid division by zero\n",
        "\n",
        "def dale_chall_score(text):\n",
        "    return textstat.dale_chall_readability_score(text)\n",
        "\n",
        "def flesch_reading_ease(text):\n",
        "    return textstat.flesch_reading_ease(text)\n",
        "\n",
        "def stopword_count(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return sum(1 for word in word_tokenize(text) if word in stop_words)\n",
        "\n",
        "def word_entropy(text):\n",
        "    words = word_tokenize(text)\n",
        "    word_freq = Counter(words)\n",
        "    probs = np.array(list(word_freq.values())) / len(words)\n",
        "    return entropy(probs)\n",
        "\n",
        "def pos_counts(text):\n",
        "    pos_tags = pos_tag(word_tokenize(text))\n",
        "    counts = Counter(tag for _, tag in pos_tags)\n",
        "    return counts\n",
        "\n",
        "def count_pronouns(text):\n",
        "    doc = nlp(text)\n",
        "    return sum(1 for token in doc if token.pos_ == \"PRON\")\n",
        "\n",
        "def count_negations(text):\n",
        "    negation_words = {\"not\", \"never\", \"none\", \"n't\"}\n",
        "    return sum(1 for word in word_tokenize(text) if word.lower() in negation_words)\n",
        "\n",
        "def count_modals(text):\n",
        "    modal_words = {\"should\", \"could\", \"might\", \"must\"}\n",
        "    return sum(1 for word in word_tokenize(text) if word.lower() in modal_words)\n",
        "\n",
        "def count_hedges(text):\n",
        "    hedge_words = {\"maybe\", \"perhaps\", \"kind of\", \"sort of\"}\n",
        "    return sum(1 for word in word_tokenize(text) if word.lower() in hedge_words)\n",
        "\n",
        "def dependency_features(text):\n",
        "    doc = nlp(text)\n",
        "    noun_count = sum(1 for token in doc if token.pos_ == \"NOUN\")\n",
        "    verb_count = sum(1 for token in doc if token.pos_ == \"VERB\")\n",
        "    dependent_clauses = sum(1 for token in doc if token.dep_ in {\"acl\", \"advcl\"})\n",
        "    return noun_count, verb_count, dependent_clauses\n",
        "\n",
        "def punctuation_features(text):\n",
        "    question_count = text.count(\"?\")\n",
        "    consecutive_punctuation = len(re.findall(r\"[!?]{2,}\", text))\n",
        "    return question_count, consecutive_punctuation\n",
        "\n",
        "df[\"pronoun_count\"] = df[\"clean_headline\"].apply(count_pronouns)\n",
        "df[\"negation_count\"] = df[\"clean_headline\"].apply(count_negations)\n",
        "df[\"modal_verbs_count\"] = df[\"clean_headline\"].apply(count_modals)\n",
        "df[\"hedge_word_count\"] = df[\"clean_headline\"].apply(count_hedges)\n",
        "#df[[\"noun_count\", \"verb_count\", \"dependent_clauses\"]] = df[\"clean_headline\"].apply(lambda x: pd.Series(dependency_features(x)))\n",
        "df[[\"question_count\", \"consecutive_punctuation\"]] = df[\"clean_headline\"].apply(lambda x: pd.Series(punctuation_features(x)))\n",
        "\n",
        "# Display first few rows\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2dROJvC1vKG"
      },
      "source": [
        "### Linguistics standalone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PkpkiKcnioL"
      },
      "outputs": [],
      "source": [
        "def combine_features(row):\n",
        "    additional_feats = np.array([\n",
        "        row['text_length'],\n",
        "        row['ner_count'],\n",
        "        row['noun_count'],\n",
        "        row['verb_count'],\n",
        "        row['adj_count'],\n",
        "        row['adv_count']\n",
        "    ], dtype=float)\n",
        "\n",
        "    # Concatenate the GloVe vector with the additional features ..or dont and just test the linguistics to see what helps\n",
        "    return np.concatenate([additional_feats])\n",
        "\n",
        "df['combined_features'] = df.apply(combine_features, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQYElTxNoX6_"
      },
      "outputs": [],
      "source": [
        "X = np.stack(df['combined_features'].values, axis=0)\n",
        "y = df['is_sarcastic'].values\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, y, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "X_train_idf = X_train\n",
        "X_test_idf = X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb3oEfGaue1z"
      },
      "outputs": [],
      "source": [
        "None+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9glY7IRzL6R"
      },
      "source": [
        "### Linguistics and tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFXQ9JkszHUF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF vectorization\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(df['clean_headline'])\n",
        "\n",
        "# Extract and scale additional features\n",
        "scaler = StandardScaler()\n",
        "additional_features = scaler.fit_transform(df[['text_length', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'dale_chall_score', 'sentiment_score', 'char_count', 'capital_char_count', 'capital_word_count', 'stopword_count', 'stopwords_vs_words', 'contrastive_marker', 'entropy', 'lexical_diversity', 'sentiment_incongruity', 'difficult_word_count']])  # now scaled\n",
        "additional_features_sparse = csr_matrix(additional_features)  # convert to sparse\n",
        "\n",
        "# Combine TF-IDF with scaled additional features\n",
        "combined_features = hstack([X_tfidf, additional_features_sparse])\n",
        "print(\"Combined shape:\", combined_features.shape)\n",
        "\n",
        "# Target\n",
        "y = df['is_sarcastic'].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    combined_features, y, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train logistic regression\n",
        "lr = LogisticRegression(class_weight='balanced', max_iter=10000)\n",
        "lr.fit(X_train, Y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = lr.predict(X_test)\n",
        "baseline_score_all_features = f1_score(Y_test, y_pred, average='macro')\n",
        "print(\"F1 Score:\", baseline_score_all_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix_jIEls7uPQ"
      },
      "source": [
        "### Feature Ablation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBCpXSW2EYGG"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# df[\"pronoun_count\"] = df[\"clean_headline\"].apply(count_pronouns)\n",
        "# df[\"negation_count\"] = df[\"clean_headline\"].apply(count_negations)\n",
        "# df[\"modal_verbs_count\"] = df[\"clean_headline\"].apply(count_modals)\n",
        "# df[\"hedge_word_count\"] = df[\"clean_headline\"].apply(count_hedges)\n",
        "# #df[[\"noun_count\", \"verb_count\", \"dependent_clauses\"]] = df[\"clean_headline\"].apply(lambda x: pd.Series(dependency_features(x)))\n",
        "# df[[\"question_count\", \"consecutive_punctuation\"]] = df[\"clean_headline\"].\n",
        "\n",
        "# ✅ Define all features you want to evaluate\n",
        "original_features = df[[\n",
        "    \"text_length\", \"ner_count\", \"noun_count\", \"verb_count\", \"adj_count\", \"adv_count\",\n",
        "    \"flesch_reading_ease\", \"dale_chall_score\", \"sentiment_score\", \"char_count\",\n",
        "    \"exclamation_count\", \"word_count\", \"capital_char_count\", \"capital_word_count\",\n",
        "    \"stopword_count\", \"avg_wordlength\", \"stopwords_vs_words\", \"contrastive_marker\",\n",
        "    'entropy', 'lexical_diversity', 'sentiment_incongruity', \"wrong_word_count\", \"difficult_word_count\", \"lengthy_word_count\"\n",
        "]]\n",
        "\n",
        "# Compute baseline model with all features\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(df['clean_headline'])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(original_features)\n",
        "scaled_features_sparse = csr_matrix(scaled_features)\n",
        "\n",
        "combined_features = hstack([X_tfidf, scaled_features_sparse])\n",
        "y = df['is_sarcastic'].values\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(combined_features, y, random_state=42, stratify=y)\n",
        "\n",
        "lr = LogisticRegression(class_weight='balanced', max_iter=10000)\n",
        "lr.fit(X_train, Y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "baseline_score_all_features = f1_score(Y_test, y_pred, average='macro')\n",
        "print(f\"Baseline Macro F1 Score: {baseline_score_all_features:.4f}\")\n",
        "\n",
        "# Feature Evaluation (Ablation Test)\n",
        "recommended_features = []\n",
        "\n",
        "for i in range(len(original_features.columns)):\n",
        "    feature = original_features.columns[i]\n",
        "\n",
        "    new_features = original_features.drop(columns=[feature])\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(new_features)\n",
        "    scaled_features_sparse = csr_matrix(scaled_features)\n",
        "\n",
        "    tfidf = TfidfVectorizer()\n",
        "    X_tfidf = tfidf.fit_transform(df['clean_headline'])\n",
        "\n",
        "    combined_features = hstack([X_tfidf, scaled_features_sparse])\n",
        "    y = df['is_sarcastic'].values\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(combined_features, y, random_state=42, stratify=y)\n",
        "\n",
        "    lr = LogisticRegression(class_weight='balanced', max_iter=10000)\n",
        "    lr.fit(X_train, Y_train)\n",
        "    y_pred = lr.predict(X_test)\n",
        "\n",
        "    new_score = f1_score(Y_test, y_pred, average='macro')\n",
        "    diff = new_score - baseline_score_all_features\n",
        "\n",
        "    if diff < 0:\n",
        "        print(f\"📈 {feature} is important (-{abs(diff):.4f})\")\n",
        "        recommended_features.append(feature)\n",
        "    elif diff > 0.001:\n",
        "        print(f\"⚠️ {feature} may be hurting performance (+{diff:.4f})\")\n",
        "    else:\n",
        "        print(f\"⚖️ {feature} is neutral ({diff:.4f})\")\n",
        "\n",
        "# Retrain model with only recommended features\n",
        "print(\"\\n Retraining with recommended features:\", recommended_features[:5], \"...\" if len(recommended_features) > 5 else \"\")\n",
        "selected_features = df[recommended_features]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(selected_features)\n",
        "scaled_features_sparse = csr_matrix(scaled_features)\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(df['clean_headline'])\n",
        "\n",
        "combined_features = hstack([X_tfidf, scaled_features_sparse])\n",
        "y = df['is_sarcastic'].values\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(combined_features, y, random_state=42, stratify=y)\n",
        "\n",
        "lr = LogisticRegression(class_weight='balanced', max_iter=10000)\n",
        "lr.fit(X_train, Y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "final_score = f1_score(Y_test, y_pred, average='macro')\n",
        "print(f\"\\n🚀 Final Macro F1 Score using best features: {final_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gj-M6hjPPhhK"
      },
      "outputs": [],
      "source": [
        "print(recommended_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80KPKoH8t7ws"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5bf7_B-LfW2"
      },
      "source": [
        "## Test finally on picked features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg16YhdR11h4",
        "outputId": "e1268d8f-7cd5-4604-c92b-97d009426407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.85      0.86      3746\n",
            "           1       0.84      0.86      0.85      3409\n",
            "\n",
            "    accuracy                           0.85      7155\n",
            "   macro avg       0.85      0.85      0.85      7155\n",
            "weighted avg       0.85      0.85      0.85      7155\n",
            "\n",
            "[[3173  573]\n",
            " [ 479 2930]]\n",
            "0.8528001655852933\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "# Prepare GloVe + linguistic features\n",
        "X_glove = np.vstack(df['word_embedding'].values)\n",
        "X_ling = df[['text_length', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'dale_chall_score',\n",
        "             'sentiment_score', 'char_count', 'capital_char_count', 'capital_word_count',\n",
        "             'stopword_count', 'stopwords_vs_words', 'contrastive_marker', 'entropy',\n",
        "             'lexical_diversity', 'sentiment_incongruity', 'difficult_word_count']].values\n",
        "\n",
        "X_gling = np.hstack([X_ling])\n",
        "y = df['is_sarcastic'].values\n",
        "\n",
        "# 👇 split while keeping indices for TF-IDF\n",
        "X_train_gling, X_test_gling, Y_train, Y_test, idx_train, idx_test = train_test_split(\n",
        "    X_gling, y, df.index, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize GloVe + linguistic features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_gling)\n",
        "X_test_scaled = scaler.transform(X_test_gling)\n",
        "\n",
        "X_train_sparse = csr_matrix(X_train_scaled)\n",
        "X_test_sparse = csr_matrix(X_test_scaled)\n",
        "\n",
        "# TF-IDF vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf_all = tfidf.fit_transform(df['clean_headline'])\n",
        "\n",
        "# ✅ Correct way to slice rows\n",
        "X_train_tfidf = X_tfidf_all[idx_train]\n",
        "X_test_tfidf = X_tfidf_all[idx_test]\n",
        "\n",
        "# Combine\n",
        "X_train_combined = hstack([X_train_tfidf, X_train_sparse])\n",
        "X_test_combined = hstack([X_test_tfidf, X_test_sparse])\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(class_weight='balanced', max_iter=10000)\n",
        "lr.fit(X_train_combined, Y_train)\n",
        "\n",
        "y_pred = lr.predict(X_test_combined)\n",
        "print(classification_report(Y_test, y_pred))\n",
        "print(confusion_matrix(Y_test, y_pred))\n",
        "print(f1_score(Y_test, y_pred, average='macro'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYLPto9zUAky"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "Sgo0hkbn_jir",
        "outputId": "85221132-3b2e-44ed-d44c-ed7b45f21fb3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=10000)</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression(class_weight='balanced', max_iter=10000)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lr = LogisticRegression(class_weight='balanced', max_iter=10000)\n",
        "lr.fit(X_train_idf, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i42JOpuQ_2Fi",
        "outputId": "1f64adc7-3e5b-428c-cc1e-1adfcc469569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.82      0.84      3746\n",
            "           1       0.81      0.86      0.83      3409\n",
            "\n",
            "    accuracy                           0.84      7155\n",
            "   macro avg       0.84      0.84      0.84      7155\n",
            "weighted avg       0.84      0.84      0.84      7155\n",
            "\n",
            "[[3059  687]\n",
            " [ 484 2925]]\n",
            "0.8362808014816456\n"
          ]
        }
      ],
      "source": [
        "y_pred = lr.predict(X_test_idf)\n",
        "print(classification_report(Y_test, y_pred))\n",
        "print(confusion_matrix(Y_test, y_pred))\n",
        "print(f1_score(Y_test, y_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "EE2uvePB89v_",
        "outputId": "ed16af36-5d59-4217-e4a9-71137d41b33c"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for +: 'NoneType' and 'int'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-dfd85c18edce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mNone\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'int'"
          ]
        }
      ],
      "source": [
        "None+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMwi1BNY2iyq"
      },
      "source": [
        "## Naive-Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SUN0TTB2m5U"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize Naive Bayes classifier\n",
        "nb = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_idf.toarray()) # Convert to dense array before scaling\n",
        "X_test_scaled = scaler.transform(X_test_idf.toarray()) # Convert to dense array before scaling\n",
        "\n",
        "\n",
        "# Train the model using the scaled features\n",
        "nb.fit(X_train_scaled, Y_train)\n",
        "\n",
        "# Make predictions using the scaled features\n",
        "y_pred = nb.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\\n\", classification_report(Y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(Y_test, y_pred))\n",
        "print(\"Macro F1 Score:\", f1_score(Y_test, y_pred, average='macro'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecHZVLpB3f2H"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wL7pjEqt3fPj"
      },
      "outputs": [],
      "source": [
        "# from sklearn.svm import LinearSVC\n",
        "\n",
        "# svm = LinearSVC(class_weight='balanced', max_iter=10000)\n",
        "# svm.fit(X_train_idf, Y_train)\n",
        "# y_pred = svm.predict(X_test_idf)\n",
        "\n",
        "# print(\"SVM F1 Score:\", f1_score(Y_test, y_pred, average='macro'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPSsXONAuik2"
      },
      "outputs": [],
      "source": [
        "None+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4Zp03dG7it9"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RZs2cHiea5gH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Ensure reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "### TF-IDF Feature Extraction\n",
        "tfidf = TfidfVectorizer()  # Limit vocab size for efficiency\n",
        "X_tfidf = tfidf.fit_transform(df['clean_headline'])\n",
        "print(f\"TF-IDF shape: {X_tfidf.shape}\")\n",
        "\n",
        "### Linguistic Features\n",
        "additional_features = df[[ \"noun_count\", \"adv_count\",\n",
        "                          \"exclamation_count\",  \"avg_wordlength\", \"stopwords_vs_words\"]].values\n",
        "\n",
        "# Standardizing the numerical features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(additional_features)\n",
        "scaled_features_sparse = csr_matrix(scaled_features)  # Convert to sparse matrix\n",
        "\n",
        "### Combine Features\n",
        "X_combined = hstack([X_tfidf, scaled_features_sparse])\n",
        "print(\"Combined features shape:\", X_combined.shape)\n",
        "\n",
        "# Target variable\n",
        "y = df['is_sarcastic'].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_combined, y, random_state=42, stratify=y)\n",
        "\n",
        "# Convert sparse matrix to dense arrays for Keras\n",
        "X_train_dense = X_train.toarray()\n",
        "X_test_dense = X_test.toarray()\n",
        "\n",
        "### Neural Network Model\n",
        "input_dim = X_train_dense.shape[1]  # Number of features\n",
        "\n",
        "# Input Layer\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "\n",
        "# # Hidden Layers\n",
        "# x = Dense(64, activation='relu')(input_layer)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = Dropout(0.5)(x)\n",
        "\n",
        "# Output Layer\n",
        "output_layer = Dense(1, activation='sigmoid')(input_layer)\n",
        "\n",
        "# Define Model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', AUC(name=\"AUC\")])\n",
        "\n",
        "# Summary\n",
        "model.summary()\n",
        "\n",
        "### Train the Model\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-6)\n",
        "\n",
        "history = model.fit(X_train_dense, Y_train,\n",
        "                    validation_data=(X_test_dense, Y_test),\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[early_stop, lr_reducer])\n",
        "\n",
        "### Evaluate Model\n",
        "loss, accuracy, auc_score = model.evaluate(X_test_dense, Y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test AUC: {auc_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AYc97nYa6No"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test_dense)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)  # Convert probabilities to binary labels (0 or 1)\n",
        "print(classification_report(Y_test, y_pred_binary))  # Use binary predictions\n",
        "print(confusion_matrix(Y_test, y_pred_binary))\n",
        "print(f1_score(Y_test, y_pred_binary, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAF-NP1wdNxD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kUDjFBDLCB0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCujPszlKJwT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
